# Установка и запуск

## Общая информация

В папке ESSE находится сам краулер для поиска и отбора текстов.
В папке get_links находятся парсеры ссылок в яндексе и гугле отдельно.

## Необходимые библиотеки

### Для запуска краулера необходима библиотека Scrapy и пара дополнительных библиотек:
```
pip install scrapy
pip install scrapy-useragents
pip install scrapy-rotating-proxies
```
### Для парсера в яндексе помимо Scrapy необходим браузер **Splash** и библиотека **scrapy-splash**
Гайд по установке Splash
https://splash.readthedocs.io/en/stable/install.html
```
pip install scrapy-splash
```
### Для парсера в гугле необходима библиотека **google**:
```
pip install google
```
## Настройка краулера
Перед запуском краулера необходимо добавить начальные ссылки для старта парсинга. Для этого в папку ESSE/links/ необходимо добавить текстовые файлы (или один файл) со ссылками через перенос строки.

В файле ESSE/ESSE/spiders/parser.py находится код краулера. В нём можно изменить количество страниц, после просмотра которых краулер остановится.(MAX_PAGES = 6000) Также можно разрешить переход на другие домены, что по умолчанию запрещено.(закоментить allowed_domains)

В файле ESSE/ESSE/settings.py настройки краулера. Файл robot.txt по умолчанию не учитывается.(ROBOTSTXT_OBEY = False) Параметр DEPTH_LIMIT = 0 обозначает глубину ссылок для парсинга(0 - парсинг ссылок на любой глубине). Также есть часть кода, отвечающая за скорость работы.
```python
DOWNLOAD_DELAY = 1
DOWNLOAD_TIMEOUT = 20
RANDOMIZE_DOWNLOAD_DELAY = True

REACTOR_THREADPOOL_MAXSIZE = 30
CONCURRENT_REQUESTS = 128
CONCURRENT_REQUESTS_PER_DOMAIN = 64
CONCURRENT_REQUESTS_PER_IP = 64

RETRY_ENABLED = True
RETRY_TIMES = 3
RETRY_HTTP_CODES = [500, 502, 503, 504, 400, 401, 403, 404, 405, 406, 407, 408, 409, 410, 429]
```


В файле ESSE/data/old_links.txt находятся ссылки, поиск текстов на которых пропускается (ссылки для перехода с них всё равно собираются). В этом файле ссылки с собраных 500 текстов. Если с сайта добавляются новые тексты, то ссылка добавляется в этот файл.

## Запуск краулера
После настройки для запуска необходимо перейти в папку ESSE в терминале. Затем запустить краулер командой:
```
scrapy crawl ESSE 
```
Найденные тексты, удовлетворяющие условиям в фильтрах, добавляются в папку ESSE/scraped_data во время работы краулера.

## Запуск парсеров ссылок
### Запуск яндекс парсера
В одной консоли запускается Splash
```
sudo docker run -p 8050:8050 scrapinghub/splash
```
После этого в файле get_links/yandex/link.txt необходимо написать запрос.Затем во второй консоли нужно перейти в папку get_links/yandex и запустить парсер:
```
scrapy crawl links
```
Будет считано 25 страниц ссылок. Ссылки с каждой страницы записываются в отдельные файлы в get_links/yandex/scraped_links. (Длится пару минут)
### Запуск google парсера
Необходимо перейти в папку get_links/google и запустить google_links.py. Затем написать запрос. Пример:
```
python3 google_links.py
advantages of zoos
```
В get_links/google/scraped_links будет записан файл с 100 ссылок.(количество ссылок можно изменить в параметре stop = 100)
В файле google_links.py записаны запросы, с помощью которых я отбирал тексты

# Принцип работы и настройка фильтров

В файле ESSE/ESSE/spiders/parser.py считываются все текстовые узлы и помещаются в массив.
Первый фильтр отфильтровывает предложения из всего текста и убирает повторы(проверяет на наличие знаков препинания в конце предложения, и кол-во слов в предложении >= 3)
Второй фильтр в полученных блоках текста ищет последовательные наборы блоков, которые соответстуют следующим условиям:
- количество слов от 150 до 280
- количество совпадений по общим словам клише >= 3
- наличие клише заключения в последней трети текста, но не в первой половине текста(по-умолчанию выключено)
- наличие клише выражающее мнение в 2/3 текста.

Общие клише в файле ESSE/data/cliches.txt
Клише выражающие мнение в файле ESSE/data/opinion_cliches.txt
Клише заключения в ESSE/data/conclusion_cliches.txt
(Клише записаны в нижнем регистре. Их также можно заменить.)

Количество слов можно изменить в ESSE/second_filter.py. Также в этом файле можно по желанию убрать некоторые фильтры клише в строке:
```python
if opinion_part_filter(text,main) and conclusion_filter(text, conclusion, stop = True):
```












